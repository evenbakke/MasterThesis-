{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/evenbakke/Documents/Master Thesis/MasterThesis-/Data 2.0/Final Data with 2024.xlsx')\n",
    "df.set_index(\"DateTime\", inplace=True)\n",
    "df.sort_index(ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc['2023':'2024']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'System Price Lag 1', 'System Price Lag 2', 'System Price Lag 3',\n",
    "    'System Price Lag 24', 'System Price Lag 48', 'System Price Lag 168'\n",
    "]\n",
    "df = df.drop(columns=columns_to_remove)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Split the original data into training and test sets\n",
    "X = df.drop('System Price', axis=1)\n",
    "y = df['System Price']\n",
    "\n",
    "X_train = X[:'2023-12-31']\n",
    "y_train = y[:'2023-12-31']\n",
    "X_test = X['2024-01-01':]\n",
    "y_test = y['2024-01-01':]\n",
    "\n",
    "# Fit the scaler on the entire training set\n",
    "dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaler_tanh.fit(X_train[non_dummy_columns])\n",
    "scaler_target_tanh.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = scaler_tanh.transform(X_train[non_dummy_columns])\n",
    "X_test_tanh = scaler_tanh.transform(X_test[non_dummy_columns])\n",
    "y_train_tanh = scaler_target_tanh.transform(y_train.values.reshape(-1, 1))\n",
    "y_test_tanh = scaler_target_tanh.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train.index)\n",
    "X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "X_train_tanh = pd.concat([X_train_tanh, X_train[dummy_columns]], axis=1)\n",
    "X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "\n",
    "y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train.index)\n",
    "y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Compute mutual information between each feature and the target variable\n",
    "mutual_info = mutual_info_regression(X_train_tanh, y_train_tanh)\n",
    "\n",
    "# Create a DataFrame to display the scores\n",
    "mi_scores = pd.DataFrame(mutual_info, index=X_train_tanh.columns, columns=['Mutual Information'])\n",
    "mi_scores = mi_scores.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "\n",
    "print(mi_scores)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mi_scores.index, y=mi_scores['Mutual Information'], palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Mutual Information Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "feature_correlation_matrix = X_train_tanh.corr().abs()\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "\n",
    "high_correlation_pairs = feature_correlation_matrix.unstack().reset_index()\n",
    "high_correlation_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "high_correlation_pairs = high_correlation_pairs[high_correlation_pairs['Feature1'] != 'Feature2']  \n",
    "high_correlation_pairs = high_correlation_pairs[high_correlation_pairs['Correlation'] > threshold]\n",
    "\n",
    "\n",
    "high_correlation_pairs = high_correlation_pairs[high_correlation_pairs['Feature1'] < high_correlation_pairs['Feature2']]\n",
    "\n",
    "\n",
    "print(\"High Correlation Pairs (threshold > 0.8):\")\n",
    "print(high_correlation_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_remove = [\n",
    "    'NO2 Price', 'NO5 Price',\n",
    "    'NO4 Price', 'SE1 Price', 'SE4 Price',\n",
    "    'DK2 Price',\n",
    "    'Coal',\n",
    "    'System Price Lag 48',\n",
    "     'Settled wind production SE', 'Settled wind production DK',\n",
    "       'Settled wind production FI', 'Settled wind production NO', \n",
    "       'Temp NO'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=features_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('System Price', axis=1)\n",
    "y = df['System Price']\n",
    "\n",
    "X_train = X[:'2023-12-31']\n",
    "y_train = y[:'2023-12-31']\n",
    "X_test = X['2024-01-01':]\n",
    "y_test = y['2024-01-01':]\n",
    "\n",
    "# Fit the scaler on the entire training set\n",
    "dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaler_tanh.fit(X_train[non_dummy_columns])\n",
    "scaler_target_tanh.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = scaler_tanh.transform(X_train[non_dummy_columns])\n",
    "X_test_tanh = scaler_tanh.transform(X_test[non_dummy_columns])\n",
    "y_train_tanh = scaler_target_tanh.transform(y_train.values.reshape(-1, 1))\n",
    "y_test_tanh = scaler_target_tanh.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train.index)\n",
    "X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "X_train_tanh = pd.concat([X_train_tanh, X_train[dummy_columns]], axis=1)\n",
    "X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "\n",
    "y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train.index)\n",
    "y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split/Employing 6-Fold Cross-Validation/Normalizing for sigmoid & tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Split the original data into training and test sets\n",
    "X = df.drop('System Price', axis=1)\n",
    "y = df['System Price']\n",
    "\n",
    "\n",
    "X_train = X[:'2023-12-31']\n",
    "y_train = y[:'2023-12-31']\n",
    "X_test = X['2024-01-01':]\n",
    "y_test = y['2024-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create rolling folds within the training set\n",
    "def create_rolling_folds(X, y, train_months, val_months):\n",
    "    folds = []\n",
    "    start_date = X.index[0]\n",
    "    total_days = X.shape[0]\n",
    "\n",
    "   \n",
    "    days_in_month = total_days // 12\n",
    "\n",
    "    for start_month in range(0, total_days - (train_months + val_months) * days_in_month + 1, days_in_month):\n",
    "        end_train_month = start_month + train_months * days_in_month\n",
    "        end_val_month = end_train_month + val_months * days_in_month\n",
    "\n",
    "        X_train_fold = X.iloc[start_month:end_train_month]\n",
    "        y_train_fold = y.iloc[start_month:end_train_month]\n",
    "        X_val_fold = X.iloc[end_train_month:end_val_month]\n",
    "        y_val_fold = y.iloc[end_train_month:end_val_month]\n",
    "\n",
    "        folds.append((X_train_fold, y_train_fold, X_val_fold, y_val_fold))\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "folds = create_rolling_folds(X_train, y_train, 6, 1)\n",
    "\n",
    "# Function to scale the data within each fold for sigmoid\n",
    "def scale_fold_sigmoid(X_train_fold, X_val_fold, y_train_fold, y_val_fold):\n",
    "    dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "    non_dummy_columns = [col for col in X_train_fold.columns if col not in dummy_columns]\n",
    "\n",
    "    scaler_sigmoid = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_target_sigmoid = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler_sigmoid.fit(X_train_fold[non_dummy_columns])\n",
    "    scaler_target_sigmoid.fit(y_train_fold.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_sigmoid = scaler_sigmoid.transform(X_train_fold[non_dummy_columns])\n",
    "    X_val_sigmoid = scaler_sigmoid.transform(X_val_fold[non_dummy_columns])\n",
    "    y_train_sigmoid = scaler_target_sigmoid.transform(y_train_fold.values.reshape(-1, 1))\n",
    "    y_val_sigmoid = scaler_target_sigmoid.transform(y_val_fold.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_sigmoid = pd.DataFrame(X_train_sigmoid, columns=non_dummy_columns, index=X_train_fold.index)\n",
    "    X_val_sigmoid = pd.DataFrame(X_val_sigmoid, columns=non_dummy_columns, index=X_val_fold.index)\n",
    "    X_train_sigmoid = pd.concat([X_train_sigmoid, X_train_fold[dummy_columns]], axis=1)\n",
    "    X_val_sigmoid = pd.concat([X_val_sigmoid, X_val_fold[dummy_columns]], axis=1)\n",
    "\n",
    "    y_train_sigmoid = pd.Series(y_train_sigmoid.flatten(), index=y_train_fold.index)\n",
    "    y_val_sigmoid = pd.Series(y_val_sigmoid.flatten(), index=y_val_fold.index)\n",
    "\n",
    "    return X_train_sigmoid, X_val_sigmoid, y_train_sigmoid, y_val_sigmoid\n",
    "\n",
    "# Function to scale the data within each fold for tanh\n",
    "def scale_fold_tanh(X_train_fold, X_val_fold, y_train_fold, y_val_fold):\n",
    "    dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "    non_dummy_columns = [col for col in X_train_fold.columns if col not in dummy_columns]\n",
    "\n",
    "    scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    scaler_tanh.fit(X_train_fold[non_dummy_columns])\n",
    "    scaler_target_tanh.fit(y_train_fold.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_tanh = scaler_tanh.transform(X_train_fold[non_dummy_columns])\n",
    "    X_val_tanh = scaler_tanh.transform(X_val_fold[non_dummy_columns])\n",
    "    y_train_tanh = scaler_target_tanh.transform(y_train_fold.values.reshape(-1, 1))\n",
    "    y_val_tanh = scaler_target_tanh.transform(y_val_fold.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train_fold.index)\n",
    "    X_val_tanh = pd.DataFrame(X_val_tanh, columns=non_dummy_columns, index=X_val_fold.index)\n",
    "    X_train_tanh = pd.concat([X_train_tanh, X_train_fold[dummy_columns]], axis=1)\n",
    "    X_val_tanh = pd.concat([X_val_tanh, X_val_fold[dummy_columns]], axis=1)\n",
    "\n",
    "    y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train_fold.index)\n",
    "    y_val_tanh = pd.Series(y_val_tanh.flatten(), index=y_val_fold.index)\n",
    "\n",
    "    return X_train_tanh, X_val_tanh, y_train_tanh, y_val_tanh\n",
    "\n",
    "# Apply scaling within each fold and store results for sigmoid\n",
    "scaled_folds_sigmoid = []\n",
    "for X_train_fold, y_train_fold, X_val_fold, y_val_fold in folds:\n",
    "    X_train_sigmoid, X_val_sigmoid, y_train_sigmoid, y_val_sigmoid = scale_fold_sigmoid(X_train_fold, X_val_fold, y_train_fold, y_val_fold)\n",
    "    scaled_folds_sigmoid.append((X_train_sigmoid, y_train_sigmoid, X_val_sigmoid, y_val_sigmoid))\n",
    "\n",
    "# Apply scaling within each fold and store results for tanh\n",
    "scaled_folds_tanh = []\n",
    "for X_train_fold, y_train_fold, X_val_fold, y_val_fold in folds:\n",
    "    X_train_tanh, X_val_tanh, y_train_tanh, y_val_tanh = scale_fold_tanh(X_train_fold, X_val_fold, y_train_fold, y_val_fold)\n",
    "    scaled_folds_tanh.append((X_train_tanh, y_train_tanh, X_val_tanh, y_val_tanh))\n",
    "\n",
    "# Function to scale the test set for sigmoid\n",
    "def scale_test_set_sigmoid(X_train, y_train, X_test, y_test):\n",
    "    dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "    non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "    scaler_sigmoid = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_target_sigmoid = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler_sigmoid.fit(X_train[non_dummy_columns])\n",
    "    scaler_target_sigmoid.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_sigmoid = scaler_sigmoid.transform(X_test[non_dummy_columns])\n",
    "    y_test_sigmoid = scaler_target_sigmoid.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_sigmoid = pd.DataFrame(X_test_sigmoid, columns=non_dummy_columns, index=X_test.index)\n",
    "    X_test_sigmoid = pd.concat([X_test_sigmoid, X_test[dummy_columns]], axis=1)\n",
    "    y_test_sigmoid = pd.Series(y_test_sigmoid.flatten(), index=y_test.index)\n",
    "\n",
    "    return X_test_sigmoid, y_test_sigmoid\n",
    "\n",
    "# Function to scale the test set for tanh\n",
    "def scale_test_set_tanh(X_train, y_train, X_test, y_test):\n",
    "    dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "    non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "    scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    scaler_tanh.fit(X_train[non_dummy_columns])\n",
    "    scaler_target_tanh.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_tanh = scaler_tanh.transform(X_test[non_dummy_columns])\n",
    "    y_test_tanh = scaler_target_tanh.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "    X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "    y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)\n",
    "\n",
    "    return X_test_tanh, y_test_tanh\n",
    "\n",
    "# Normalize the test set for sigmoid\n",
    "X_test_sigmoid, y_test_sigmoid = scale_test_set_sigmoid(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Normalize the test set for tanh\n",
    "X_test_tanh, y_test_tanh = scale_test_set_tanh(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def visualize_folds(folds, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(14, 15))\n",
    "    for i, (X_train_fold, y_train_fold, X_val_fold, y_val_fold) in enumerate(folds):\n",
    "        plt.subplot(len(folds), 1, i + 1)\n",
    "        plt.plot(y_train_fold.index, y_train_fold, 'blue', label='Training Data')\n",
    "        plt.plot(y_val_fold.index, y_val_fold, 'red', label='Validation Data')\n",
    "        plt.title(f'Fold {i+1} {title_suffix}')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_folds(scaled_folds_sigmoid, title_suffix=\"(Scaled for Sigmoid)\")\n",
    "\n",
    "\n",
    "visualize_folds(scaled_folds_tanh, title_suffix=\"(Scaled for Tanh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - tanh activation function in output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of model & hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Initializing hidden layers\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Activation function for hidden layers\n",
    "        self.activation = nn.ReLU()  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        \n",
    "        x = torch.tanh(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def calculate_loss(predictions, targets, criterion):\n",
    "    return criterion(predictions, targets).item()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    return epoch_losses\n",
    "\n",
    "def validate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            preds = model(X_val)\n",
    "            val_loss = criterion(preds, y_val)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "    return epoch_val_losses\n",
    "\n",
    "def initialize_tensorboard(path):\n",
    "    return SummaryWriter(path)\n",
    "\n",
    "hyperparam_space = {\n",
    "    'learning_rate': [0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'num_hidden_layers': [1, 2, 3, 4],\n",
    "    'hidden_size': [64, 128, 256, 512],\n",
    "    'momentum': [0.0, 0.5, 0.9]  \n",
    "}\n",
    "\n",
    "# Main function to run training with random search\n",
    "def run_random_search(folds, input_size, num_trials):\n",
    "    writer = initialize_tensorboard('runs/electricity_price_hyperparameter_search_tanh')\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparams = {}\n",
    "    num_epochs = 30  \n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        lr = random.choice(hyperparam_space['learning_rate'])\n",
    "        batch_size = random.choice(hyperparam_space['batch_size'])\n",
    "        num_hidden_layers = random.choice(hyperparam_space['num_hidden_layers'])\n",
    "        hidden_size = random.choice(hyperparam_space['hidden_size'])\n",
    "        momentum = random.choice(hyperparam_space['momentum'])\n",
    "\n",
    "        fold_val_losses = []\n",
    "\n",
    "        for i, (X_train_fold, y_train_fold, X_test_fold, y_test_fold) in enumerate(folds):\n",
    "            model = MLP(input_size, hidden_size, num_hidden_layers)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "            train_dataset = TensorDataset(torch.tensor(X_train_fold.values, dtype=torch.float32),\n",
    "                                          torch.tensor(y_train_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            test_dataset = TensorDataset(torch.tensor(X_test_fold.values, dtype=torch.float32),\n",
    "                                         torch.tensor(y_test_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_train_losses = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "                avg_train_loss = np.mean(epoch_train_losses)\n",
    "\n",
    "                epoch_val_losses = validate(model, test_loader, criterion)\n",
    "                avg_val_loss = np.mean(epoch_val_losses)\n",
    "                fold_val_losses.append(avg_val_loss)\n",
    "\n",
    "                writer.add_scalar(f'Trial_{trial}_Fold_{i+1}/Loss/Train', avg_train_loss, epoch)\n",
    "                writer.add_scalar(f'Trial_{trial}_Fold_{i+1}/Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "                \n",
    "                scheduler.step()\n",
    "\n",
    "            avg_fold_val_loss = np.mean(fold_val_losses)\n",
    "            if avg_fold_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_fold_val_loss\n",
    "                best_hyperparams = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_hidden_layers': num_hidden_layers,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'momentum': momentum,\n",
    "                }\n",
    "            print(f\"Trial {trial} | Fold {i+1} completed | Average Train Loss: {avg_train_loss:.6f} | Average Validation Loss: {avg_fold_val_loss:.6f}\")\n",
    "        \n",
    "    writer.close()\n",
    "    print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.6f}\")\n",
    "    return best_hyperparams, best_val_loss\n",
    "\n",
    "\n",
    "input_size = X_train_tanh.shape[1]  \n",
    "num_trials = 20  \n",
    "best_hyperparams, best_val_loss = run_random_search(scaled_folds_tanh, input_size, num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using CV/Monitoring validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def calculate_loss(predictions, targets, criterion):\n",
    "    return criterion(predictions, targets).item()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    return epoch_losses\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            preds = model(X_val)\n",
    "            val_loss = criterion(preds, y_val)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "    return epoch_val_losses\n",
    "\n",
    "def initialize_tensorboard(path):\n",
    "    return SummaryWriter(path)\n",
    "\n",
    "# Main function to run training with cross-validation\n",
    "def run_training(scaled_folds_tanh, input_size, hidden_size, num_hidden_layers, lr, batch_size, num_epochs, momentum, patience):\n",
    "    writer = initialize_tensorboard('runs/electricity_price_forecasting_tanh')\n",
    "    all_fold_train_losses = []\n",
    "    all_fold_val_losses = []\n",
    "\n",
    "    avg_train_losses_per_fold = []\n",
    "    avg_val_losses_per_fold = []\n",
    "\n",
    "    for i, (X_train_fold, y_train_fold, X_val_fold, y_val_fold) in enumerate(scaled_folds_tanh):\n",
    "        model = MLP(input_size, hidden_size, num_hidden_layers)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_fold.values, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val_fold.values, dtype=torch.float32),\n",
    "                                    torch.tensor(y_val_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        fold_train_losses = []\n",
    "        fold_val_losses = []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_train_losses = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "            avg_train_loss = np.mean(epoch_train_losses)\n",
    "\n",
    "            epoch_val_losses = validate(model, val_loader, criterion)\n",
    "            avg_val_loss = np.mean(epoch_val_losses)\n",
    "\n",
    "            fold_train_losses.append(avg_train_loss)\n",
    "            fold_val_losses.append(avg_val_loss)\n",
    "\n",
    "            writer.add_scalar(f'Fold_{i+1}/Loss/Train', avg_train_loss, epoch)\n",
    "            writer.add_scalar(f'Fold_{i+1}/Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "            print(f\"Fold {i+1} | Epoch: {epoch+1:03d}/{num_epochs} | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for fold {i+1}\")\n",
    "                break\n",
    "\n",
    "        all_fold_train_losses.append(fold_train_losses)\n",
    "        all_fold_val_losses.append(fold_val_losses)\n",
    "\n",
    "        avg_train_losses_per_fold.append(np.mean(fold_train_losses))\n",
    "        avg_val_losses_per_fold.append(np.mean(fold_val_losses))\n",
    "\n",
    "        print(f\"Fold {i+1} completed | Average Train Loss: {np.mean(fold_train_losses):.6f} | Average Validation Loss: {np.mean(fold_val_losses):.6f}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    print(\"Average Train Loss per Fold:\", avg_train_losses_per_fold)\n",
    "    print(\"Average Validation Loss per Fold:\", avg_val_losses_per_fold)\n",
    "    print(f\"Overall Average Train Loss: {np.mean(avg_train_losses_per_fold):.6f}\")\n",
    "    print(f\"Overall Average Validation Loss: {np.mean(avg_val_losses_per_fold):.6f}\")\n",
    "\n",
    "    return all_fold_train_losses, all_fold_val_losses\n",
    "\n",
    "# Define hyperparameters \n",
    "input_size = X_train_tanh.shape[1]\n",
    "hidden_size = 64\n",
    "num_hidden_layers = 2\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "momentum = 0.9  \n",
    "\n",
    "# Early stopping parameter\n",
    "patience = 100  # no early stopping when setting too high\n",
    "\n",
    "all_fold_train_losses, all_fold_val_losses = run_training(scaled_folds_tanh, input_size, hidden_size, num_hidden_layers, learning_rate, batch_size, num_epochs, momentum, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting the train and validation loss over epochs for each fold\n",
    "for i in range(len(folds)):\n",
    "    num_epochs_fold = len(all_fold_train_losses[i])  \n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    plt.plot(range(1, num_epochs_fold + 1), all_fold_train_losses[i], label='Train Loss', color='blue', linewidth=2)\n",
    "    plt.plot(range(1, num_epochs_fold + 1), all_fold_val_losses[i], label='Validation Loss', color='orange', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} Train and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)  \n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_epochs = max([len(fold_losses) for fold_losses in all_fold_train_losses])\n",
    "\n",
    "train_losses_accum = np.zeros(max_epochs)\n",
    "val_losses_accum = np.zeros(max_epochs)\n",
    "count_per_epoch = np.zeros(max_epochs)  \n",
    "\n",
    "for i in range(len(all_fold_train_losses)):\n",
    "    num_epochs_fold = len(all_fold_train_losses[i])\n",
    "    for epoch in range(num_epochs_fold):\n",
    "        train_losses_accum[epoch] += all_fold_train_losses[i][epoch]\n",
    "        val_losses_accum[epoch] += all_fold_val_losses[i][epoch]\n",
    "        count_per_epoch[epoch] += 1\n",
    "\n",
    "avg_train_losses = train_losses_accum / count_per_epoch\n",
    "avg_val_losses = val_losses_accum / count_per_epoch\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_epochs + 1), avg_train_losses, label='Average Train Loss', color='blue', linewidth=2)\n",
    "plt.plot(range(1, max_epochs + 1), avg_val_losses, label='Average Validation Loss', color='orange', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average Train and Validation Loss Across All Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on all training data with the optimal hyperparameters found in CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Normalize all of the training data together to ensure consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the entire training set\n",
    "dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaler_tanh.fit(X_train[non_dummy_columns])\n",
    "scaler_target_tanh.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = scaler_tanh.transform(X_train[non_dummy_columns])\n",
    "X_test_tanh = scaler_tanh.transform(X_test[non_dummy_columns])\n",
    "y_train_tanh = scaler_target_tanh.transform(y_train.values.reshape(-1, 1))\n",
    "y_test_tanh = scaler_target_tanh.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train.index)\n",
    "X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "X_train_tanh = pd.concat([X_train_tanh, X_train[dummy_columns]], axis=1)\n",
    "X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "\n",
    "y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train.index)\n",
    "y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tanh.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Training Loop (training on all of 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=80):\n",
    "    model.train()\n",
    "    average_losses_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        average_losses_per_epoch.append(avg_loss)\n",
    "        print(f'Epoch: {epoch+1}, Average Training Loss: {avg_loss:.6f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_tanh.values, dtype=torch.float32),\n",
    "                              torch.tensor(y_train_tanh.values, dtype=torch.float32).view(-1, 1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)  \n",
    "\n",
    "input_size = X_train_tanh.shape[1]\n",
    "model = MLP(input_size, hidden_size=64, num_hidden_layers=2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "trained_model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=100)\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'final_trained_model.pth')\n",
    "print(\"Training complete and model saved.\")\n",
    "\n",
    "input_size = X_train_tanh.shape[1]\n",
    "hidden_size = 64\n",
    "num_hidden_layers = 2\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing loop, recalibration loop, storing predictions & actuals, error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import timedelta\n",
    "import calendar\n",
    "\n",
    "def rescale_values(scaler, values):\n",
    "    return scaler.inverse_transform(values)\n",
    "\n",
    "def calculate_metrics(predictions, actuals, epsilon=1e-10, small_value=1e-5):\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    safe_actuals = np.where(np.abs(actuals) < small_value, small_value, np.abs(actuals))\n",
    "    mape = np.mean(np.abs((actuals - predictions) / safe_actuals)) * 100\n",
    "    \n",
    "    return mse, rmse, mae, mape\n",
    "\n",
    "def calculate_smape(predictions, actuals, epsilon=1e-10):\n",
    "    smape = 100 * np.mean(2 * np.abs(predictions - actuals) / (np.abs(actuals) + np.abs(predictions) + epsilon))\n",
    "    return smape\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    test_days = len(y_test) // 24  \n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    all_dates = []\n",
    "\n",
    "    for day in range(test_days):\n",
    "        start_idx = day * 24\n",
    "        end_idx = (day + 1) * 24\n",
    "        X_day = torch.tensor(X_test.iloc[start_idx:end_idx].values, dtype=torch.float32)\n",
    "        y_day = y_test.iloc[start_idx:end_idx].values\n",
    "        y_dates = y_test.iloc[start_idx:end_idx].index\n",
    "\n",
    "        # Forecast the next 24 hours\n",
    "        with torch.no_grad():\n",
    "            y_pred_day = model(X_day).numpy()\n",
    "\n",
    "        # Store normalized predictions and actual values with their dates\n",
    "        all_predictions.extend(y_pred_day.flatten())\n",
    "        all_actuals.extend(y_day.flatten())\n",
    "        all_dates.extend(y_dates)\n",
    "\n",
    "    # Convert the lists to numpy arrays \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    all_dates = pd.to_datetime(all_dates)\n",
    "\n",
    "    return all_predictions, all_actuals, all_dates\n",
    "\n",
    "\n",
    "# Function to update training and testing data\n",
    "def update_training_data(X, y, start_date, end_date):\n",
    "    X_train = X[start_date:end_date]\n",
    "    y_train = y[start_date:end_date]\n",
    "    return X_train, y_train\n",
    "\n",
    "# Function to get the first day of the next month\n",
    "def next_month(date):\n",
    "    year = date.year + (date.month // 12)\n",
    "    month = (date.month % 12) + 1\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "# Function to get the last day of the month\n",
    "def last_day_of_month(date):\n",
    "    next_month_date = next_month(date)\n",
    "    return next_month_date - timedelta(days=1)\n",
    "\n",
    "# Recalibration loop\n",
    "def recalibration_loop(X, y, start_date, num_months, model, input_size, hidden_size, num_hidden_layers, lr, batch_size, num_epochs, momentum, scaler_X, scaler_y):\n",
    "    initial_end_date = last_day_of_month(start_date + timedelta(days=364))  \n",
    "    test_start_date = initial_end_date + timedelta(days=1)  \n",
    "\n",
    "    # Store normalized predictions and actual values for each month\n",
    "    all_normalized_predictions = []\n",
    "    all_normalized_actuals = []\n",
    "    all_dates = []\n",
    "    all_performance_metrics = []  \n",
    "\n",
    "    for month in range(num_months):\n",
    "        test_end_date = last_day_of_month(test_start_date) + timedelta(hours=23)  \n",
    "\n",
    "        print(f'\\nRecalibration month {month + 1}...')\n",
    "        print(f'Training period: {start_date.date()} to {initial_end_date.date()}')\n",
    "        print(f'Testing period: {test_start_date.date()} to {test_end_date.date()}')\n",
    "\n",
    "       \n",
    "        if test_start_date >= test_end_date or len(X[test_start_date:test_end_date]) == 0:\n",
    "            print(f'Skipping month {month + 1} due to invalid test period.')\n",
    "            break\n",
    "\n",
    "        X_train, y_train = update_training_data(X, y, start_date, initial_end_date)\n",
    "        \n",
    "        non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "        X_train_tanh = scaler_X.transform(X_train[non_dummy_columns])\n",
    "        y_train_tanh = scaler_y.transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train.index)\n",
    "        X_train_tanh = pd.concat([X_train_tanh, X_train[dummy_columns]], axis=1)\n",
    "        y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train.index)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_tanh.values, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train_tanh.values, dtype=torch.float32).view(-1, 1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = MLP(input_size, hidden_size, num_hidden_layers)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        trained_model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "        X_test = X[test_start_date:test_end_date]\n",
    "        y_test = y[test_start_date:test_end_date]\n",
    "\n",
    "        X_test_tanh = scaler_X.transform(X_test[non_dummy_columns])\n",
    "        y_test_tanh = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "        X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "        X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "        y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)\n",
    "\n",
    "        normalized_predictions, normalized_actuals, dates = test_model(trained_model, X_test_tanh, y_test_tanh)\n",
    "        all_normalized_predictions.extend(normalized_predictions)\n",
    "        all_normalized_actuals.extend(normalized_actuals)\n",
    "        all_dates.extend(dates)\n",
    "\n",
    "        # Calculate performance metrics for the test month\n",
    "        mse, rmse, mae, mape = calculate_metrics(normalized_predictions, normalized_actuals)\n",
    "        smape = calculate_smape(normalized_predictions, normalized_actuals)\n",
    "        all_performance_metrics.append((mse, rmse, mae, mape, smape))\n",
    "\n",
    "        print(f'Month {month + 1} - Test Period: {test_start_date.date()} to {test_end_date.date()}')\n",
    "        print(f'Overall MSE: {mse:.6f}')\n",
    "        print(f'Overall RMSE: {rmse:.6f}')\n",
    "        print(f'Overall MAE: {mae:.6f}')\n",
    "        print(f'Overall MAPE: {mape:.6f}%')\n",
    "        print(f'Overall SMAPE: {smape:.6f}%')\n",
    "\n",
    "        \n",
    "        start_date = next_month(start_date)\n",
    "        initial_end_date = last_day_of_month(start_date + timedelta(days=364))\n",
    "        test_start_date = initial_end_date + timedelta(days=1)\n",
    "\n",
    "    return all_normalized_predictions, all_normalized_actuals, all_performance_metrics, all_dates\n",
    "\n",
    "\n",
    "start_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "\n",
    "num_months = 5  \n",
    "\n",
    "\n",
    "normalized_predictions, normalized_actuals, all_performance_metrics, dates = recalibration_loop(X, y, start_date, num_months, model, input_size, hidden_size, num_hidden_layers, learning_rate, batch_size, num_epochs, momentum, scaler_tanh, scaler_target_tanh)\n",
    "\n",
    "\n",
    "all_predictions_rescaled = rescale_values(scaler_target_tanh, np.array(normalized_predictions).reshape(-1, 1))\n",
    "all_actuals_rescaled = rescale_values(scaler_target_tanh, np.array(normalized_actuals).reshape(-1, 1))\n",
    "\n",
    "# Create DataFrames for predictions and actuals with datetime index\n",
    "predictions_df = pd.DataFrame(all_predictions_rescaled, index=dates, columns=[\"Prediction\"])\n",
    "actuals_df = pd.DataFrame(all_actuals_rescaled, index=dates, columns=[\"Actual\"])\n",
    "\n",
    "# Combine predictions and actuals into a single DataFrame\n",
    "Predictions_actuals_MLP_tanh = pd.concat([predictions_df, actuals_df], axis=1)\n",
    "\n",
    "performance_metrics_per_month = []\n",
    "\n",
    "start_idx = 0\n",
    "total_length = len(normalized_predictions)\n",
    "\n",
    "for month in range(1, 6):  \n",
    "    test_start_date = pd.to_datetime('2024-01-01') + pd.DateOffset(months=month - 1)\n",
    "    num_days_in_month = calendar.monthrange(test_start_date.year, test_start_date.month)[1]\n",
    "    end_idx = start_idx + num_days_in_month * 24\n",
    "    \n",
    "    if end_idx > total_length:\n",
    "        end_idx = total_length\n",
    "\n",
    "    monthly_predictions = all_predictions_rescaled[start_idx:end_idx]\n",
    "    monthly_actuals = all_actuals_rescaled[start_idx:end_idx]\n",
    "\n",
    "    print(f'Month {month}:')\n",
    "    print(f'Test period: {test_start_date.strftime(\"%Y-%m-%d 00:00:00\")} to {(test_start_date + pd.DateOffset(days=num_days_in_month - 1)).strftime(\"%Y-%m-%d 23:00:00\")}')\n",
    "    print(f'Start index: {start_idx}, End index: {end_idx - 1}')\n",
    "    print(f'Length of monthly predictions: {len(monthly_predictions)}')\n",
    "    print(f'Length of monthly actuals: {len(monthly_actuals)}')\n",
    "\n",
    "    if len(monthly_predictions) != num_days_in_month * 24:\n",
    "        print(f\"Error: Expected {num_days_in_month * 24} predictions, but got {len(monthly_predictions)}\")\n",
    "\n",
    "    mse, rmse, mae, mape = calculate_metrics(monthly_predictions, monthly_actuals)\n",
    "    smape = calculate_smape(monthly_predictions, monthly_actuals)\n",
    "    performance_metrics_per_month.append({\n",
    "        'Month': month,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'SMAPE': smape\n",
    "    })\n",
    "\n",
    "    start_idx = end_idx\n",
    "\n",
    "for metrics in performance_metrics_per_month:\n",
    "    print(f'Month {metrics[\"Month\"]}: MSE={metrics[\"MSE\"]:.6f}, RMSE={metrics[\"RMSE\"]:.6f}, MAE={metrics[\"MAE\"]:.6f}, MAPE={metrics[\"MAPE\"]:.6f}%, SMAPE={metrics[\"SMAPE\"]:.6f}%')\n",
    "\n",
    "for record in Predictions_actuals_MLP_tanh.head(10).itertuples():\n",
    "    print(f'Datetime: {record.Index}, Prediction: {record.Prediction:.6f}, Actual: {record.Actual:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP predictions to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames on their datetime index\n",
    "predictions_actuals_MLP_tanh = pd.concat([predictions_df, actuals_df], axis=1)\n",
    "predictions_actuals_MLP_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions \n",
    "\n",
    "excel_file_path = '/Users/evenbakke/Documents/Master Thesis/Predictions all models /MLP predictions v2.xlsx'\n",
    "\n",
    "predictions_actuals_MLP_tanh.to_excel(excel_file_path, index=True)\n",
    "\n",
    "print(f'DataFrame successfully saved to {excel_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (tanh output layer, one lstm layer, one hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_layers, fc_hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Hidden fully connected layer\n",
    "        self.hidden_layer = nn.Linear(lstm_hidden_size, fc_hidden_size)\n",
    "        \n",
    "        # Output fully connected layer\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()  # ReLU activation function for hidden layers\n",
    "        self.tanh = nn.Tanh()  # Tanh activation function for the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size).to(x.device)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        hidden_out = self.relu(self.hidden_layer(lstm_out))\n",
    "        \n",
    "        output = self.tanh(self.output_layer(hidden_out))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def calculate_loss(predictions, targets, criterion):\n",
    "    return criterion(predictions, targets).item()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    return epoch_losses\n",
    "\n",
    "def validate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            preds = model(X_val)\n",
    "            val_loss = criterion(preds, y_val)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "    return epoch_val_losses\n",
    "\n",
    "def initialize_tensorboard(path):\n",
    "    return SummaryWriter(path)\n",
    "\n",
    "hyperparam_space = {\n",
    "    'learning_rate': [0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'hidden_size': [64, 128, 256, 512],\n",
    "    'momentum': [0.0, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Main function to run training with random search\n",
    "def run_random_search(folds, input_size, sequence_length, num_trials):\n",
    "    writer = initialize_tensorboard('runs/electricity_price_hyperparameter_search')\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparams = {}\n",
    "    num_epochs = 30  \n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        lr = random.choice(hyperparam_space['learning_rate'])\n",
    "        batch_size = random.choice(hyperparam_space['batch_size'])\n",
    "        num_layers = random.choice(hyperparam_space['num_layers'])\n",
    "        hidden_size = random.choice(hyperparam_space['hidden_size'])\n",
    "        momentum = random.choice(hyperparam_space['momentum'])\n",
    "\n",
    "        fold_val_losses = []\n",
    "\n",
    "        for i, (X_train_fold, y_train_fold, X_test_fold, y_test_fold) in enumerate(folds):\n",
    "            model = LSTMModel(input_size, hidden_size, num_layers)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "            train_dataset = TensorDataset(torch.tensor(X_train_fold.values, dtype=torch.float32).unsqueeze(1),\n",
    "                                          torch.tensor(y_train_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            test_dataset = TensorDataset(torch.tensor(X_test_fold.values, dtype=torch.float32).unsqueeze(1),\n",
    "                                         torch.tensor(y_test_fold.values, dtype=torch.float32).view(-1, 1))\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_train_losses = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "                avg_train_loss = np.mean(epoch_train_losses)\n",
    "\n",
    "                epoch_val_losses = validate(model, test_loader, criterion)\n",
    "                avg_val_loss = np.mean(epoch_val_losses)\n",
    "                fold_val_losses.append(avg_val_loss)\n",
    "\n",
    "                writer.add_scalar(f'Trial_{trial}_Fold_{i+1}/Loss/Train', avg_train_loss, epoch)\n",
    "                writer.add_scalar(f'Trial_{trial}_Fold_{i+1}/Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "        \n",
    "                scheduler.step()\n",
    "\n",
    "            avg_fold_val_loss = np.mean(fold_val_losses)\n",
    "            if avg_fold_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_fold_val_loss\n",
    "                best_hyperparams = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'momentum': momentum,\n",
    "                }\n",
    "            print(f\"Trial {trial} | Fold {i+1} completed | Average Train Loss: {avg_train_loss:.6f} | Average Validation Loss: {avg_fold_val_loss:.6f}\")\n",
    "        \n",
    "    writer.close()\n",
    "    print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.6f}\")\n",
    "    return best_hyperparams, best_val_loss\n",
    "\n",
    "input_size = X_train_tanh.shape[1]  \n",
    "sequence_length = 24 \n",
    "num_trials = 20  \n",
    "best_hyperparams, best_val_loss = run_random_search(scaled_folds_tanh, input_size, sequence_length, num_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using CV (SGD optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def calculate_loss(predictions, targets, criterion):\n",
    "    return criterion(predictions, targets).item()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    return epoch_losses\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            preds = model(X_val)\n",
    "            val_loss = criterion(preds, y_val)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "    return epoch_val_losses\n",
    "\n",
    "def create_sequences(data, targets, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data.iloc[i:(i + seq_length)].values\n",
    "        y = targets.iloc[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def initialize_tensorboard(path):\n",
    "    return SummaryWriter(path)\n",
    "\n",
    "# Main function to run training with cross-validation\n",
    "def run_training(folds, input_size, lstm_hidden_size, num_layers, fc_hidden_size, lr, batch_size, num_epochs, patience, sequence_length, momentum):\n",
    "    writer = initialize_tensorboard('runs/electricity_price_forecasting_lstm')\n",
    "    all_fold_train_losses = []\n",
    "    all_fold_val_losses = []\n",
    "\n",
    "    avg_train_losses_per_fold = []\n",
    "    avg_val_losses_per_fold = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i, (X_train_fold, y_train_fold, X_test_fold, y_test_fold) in enumerate(folds):\n",
    "        model = LSTMModel(input_size, lstm_hidden_size, num_layers, fc_hidden_size).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "        # Create sequences for the training set\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_fold, y_train_fold, sequence_length)\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_seq, dtype=torch.float32),\n",
    "                                      torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Create sequences for the test set\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_fold, y_test_fold, sequence_length)\n",
    "        test_dataset = TensorDataset(torch.tensor(X_test_seq, dtype=torch.float32),\n",
    "                                     torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        fold_train_losses = []\n",
    "        fold_val_losses = []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_train_losses = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            avg_train_loss = np.mean(epoch_train_losses)\n",
    "\n",
    "            epoch_val_losses = validate(model, test_loader, criterion, device)\n",
    "            avg_val_loss = np.mean(epoch_val_losses)\n",
    "\n",
    "            fold_train_losses.append(avg_train_loss)\n",
    "            fold_val_losses.append(avg_val_loss)\n",
    "\n",
    "            writer.add_scalar(f'Fold_{i+1}/Loss/Train', avg_train_loss, epoch)\n",
    "            writer.add_scalar(f'Fold_{i+1}/Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "            print(f\"Fold {i+1} | Epoch: {epoch+1:03d}/{num_epochs} | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = model.state_dict()  \n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for fold {i+1}\")\n",
    "                break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "        all_fold_train_losses.append(fold_train_losses)\n",
    "        all_fold_val_losses.append(fold_val_losses)\n",
    "\n",
    "        avg_train_losses_per_fold.append(np.mean(fold_train_losses))\n",
    "        avg_val_losses_per_fold.append(np.mean(fold_val_losses))\n",
    "\n",
    "        print(f\"Fold {i+1} completed | Average Train Loss: {np.mean(fold_train_losses):.6f} | Average Validation Loss: {np.mean(fold_val_losses):.6f}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    print(\"Average Train Loss per Fold:\", avg_train_losses_per_fold)\n",
    "    print(\"Average Validation Loss per Fold:\", avg_val_losses_per_fold)\n",
    "    print(f\"Overall Average Train Loss: {np.mean(avg_train_losses_per_fold):.6f}\")\n",
    "    print(f\"Overall Average Validation Loss: {np.mean(avg_val_losses_per_fold):.6f}\")\n",
    "\n",
    "    return all_fold_train_losses, all_fold_val_losses\n",
    "\n",
    "\n",
    "input_size = X_train_tanh.shape[1]\n",
    "lstm_hidden_size = 64\n",
    "num_layers = 1\n",
    "fc_hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "sequence_length = 24  \n",
    "momentum = 0.9\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 100  # no early stopping when setting too high\n",
    "\n",
    "all_fold_train_losses, all_fold_val_losses = run_training(scaled_folds_tanh, input_size, lstm_hidden_size, num_layers, fc_hidden_size, learning_rate, batch_size, num_epochs, patience, sequence_length, momentum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for i in range(len(folds)):\n",
    "    num_epochs_fold = len(all_fold_train_losses[i])  \n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    plt.plot(range(1, num_epochs_fold + 1), all_fold_train_losses[i], label='Train Loss', color='blue', linewidth=2)\n",
    "    plt.plot(range(1, num_epochs_fold + 1), all_fold_val_losses[i], label='Validation Loss', color='orange', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} Train and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True) \n",
    "    plt.show()\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_epochs = max([len(fold_losses) for fold_losses in all_fold_train_losses])\n",
    "\n",
    "train_losses_accum = np.zeros(max_epochs)\n",
    "val_losses_accum = np.zeros(max_epochs)\n",
    "count_per_epoch = np.zeros(max_epochs)  \n",
    "\n",
    "for i in range(len(all_fold_train_losses)):\n",
    "    num_epochs_fold = len(all_fold_train_losses[i])\n",
    "    for epoch in range(num_epochs_fold):\n",
    "        train_losses_accum[epoch] += all_fold_train_losses[i][epoch]\n",
    "        val_losses_accum[epoch] += all_fold_val_losses[i][epoch]\n",
    "        count_per_epoch[epoch] += 1\n",
    "\n",
    "avg_train_losses = train_losses_accum / count_per_epoch\n",
    "avg_val_losses = val_losses_accum / count_per_epoch\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_epochs + 1), avg_train_losses, label='Average Train Loss', color='blue', linewidth=2)\n",
    "plt.plot(range(1, max_epochs + 1), avg_val_losses, label='Average Validation Loss', color='orange', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average Train and Validation Loss Across All Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model (normalize all training data, testing loop, recalibration loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the entire training set\n",
    "dummy_columns = ['Weekend', 'Christmas vacation', 'Public holiday', 'Winter Time']\n",
    "non_dummy_columns = [col for col in X_train.columns if col not in dummy_columns]\n",
    "\n",
    "scaler_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_target_tanh = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaler_tanh.fit(X_train[non_dummy_columns])\n",
    "scaler_target_tanh.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = scaler_tanh.transform(X_train[non_dummy_columns])\n",
    "X_test_tanh = scaler_tanh.transform(X_test[non_dummy_columns])\n",
    "y_train_tanh = scaler_target_tanh.transform(y_train.values.reshape(-1, 1))\n",
    "y_test_tanh = scaler_target_tanh.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train.index)\n",
    "X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test.index)\n",
    "X_train_tanh = pd.concat([X_train_tanh, X_train[dummy_columns]], axis=1)\n",
    "X_test_tanh = pd.concat([X_test_tanh, X_test[dummy_columns]], axis=1)\n",
    "\n",
    "y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train.index)\n",
    "y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = X_train_tanh.shape[1]\n",
    "lstm_hidden_size = 64\n",
    "num_layers = 1\n",
    "fc_hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "sequence_length = 24\n",
    "momentum = 0.9\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i + seq_length]\n",
    "        y = target[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_tanh.values, y_train_tanh.values, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_tanh.values, y_test_tanh.values, sequence_length)\n",
    "\n",
    "X_train_seq = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_seq = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)\n",
    "X_test_seq = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_seq = torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LSTMModel(input_size=input_size, lstm_hidden_size=lstm_hidden_size, num_layers=num_layers, fc_hidden_size=fc_hidden_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    model.train()\n",
    "    average_losses_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        average_losses_per_epoch.append(avg_loss)\n",
    "        print(f'Epoch: {epoch+1}, Average Training Loss: {avg_loss:.6f}')\n",
    "    return model\n",
    "\n",
    "trained_model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'final_trained_model_lstm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model, training and recalibration loop, testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_layers, fc_hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, fc_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "def rescale_values(scaler, values):\n",
    "    return scaler.inverse_transform(values)\n",
    "\n",
    "def calculate_metrics(predictions, actuals, epsilon=1e-10, small_value=1e-5):\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    safe_actuals = np.where(np.abs(actuals) < small_value, small_value, np.abs(actuals))\n",
    "    mape = np.mean(np.abs((actuals - predictions) / safe_actuals)) * 100\n",
    "    \n",
    "    return mse, rmse, mae, mape\n",
    "\n",
    "def calculate_smape(predictions, actuals, epsilon=1e-10):\n",
    "    smape = 100 * np.mean(2 * np.abs(predictions - actuals) / (np.abs(actuals) + np.abs(predictions) + epsilon))\n",
    "    return smape\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model, X_test, y_test, sequence_length, scaler_target_tanh):\n",
    "    model.eval()\n",
    "    test_hours = len(y_test)  \n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    all_dates = []\n",
    "\n",
    "    # Initial sequence\n",
    "    X_seq = torch.tensor(X_test.iloc[:sequence_length].values, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Predictions for the first 24 hours\n",
    "    for hour in range(sequence_length):\n",
    "        y_actual = y_test.iloc[hour]\n",
    "        y_date = y_test.index[hour]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_seq).numpy()\n",
    "\n",
    "        all_predictions.append(y_pred.flatten()[0])\n",
    "        all_actuals.append(y_actual)\n",
    "        all_dates.append(y_date)\n",
    "\n",
    "        new_input = torch.tensor(X_test.iloc[hour].values, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        X_seq = torch.cat((X_seq[:, 1:, :], new_input), dim=1)\n",
    "\n",
    "    for hour in range(sequence_length, test_hours):\n",
    "        y_actual = y_test.iloc[hour]\n",
    "        y_date = y_test.index[hour]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_seq).numpy()\n",
    "\n",
    "        all_predictions.append(y_pred.flatten()[0])\n",
    "        all_actuals.append(y_actual)\n",
    "        all_dates.append(y_date)\n",
    "\n",
    "        new_input = torch.tensor(X_test.iloc[hour].values, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        X_seq = torch.cat((X_seq[:, 1:, :], new_input), dim=1)\n",
    "\n",
    "        if (hour + 1) % 24 == 0:\n",
    "            actuals_24h = y_test.iloc[hour - 23:hour + 1].values\n",
    "            X_test.iloc[hour - 23:hour + 1, -1] = scaler_target_tanh.transform(actuals_24h.reshape(-1, 1)).flatten().astype(X_test.iloc[:, -1].dtype)\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actuals = np.array(all_actuals)\n",
    "    all_dates = pd.to_datetime(all_dates)\n",
    "\n",
    "    return all_predictions, all_actuals, all_dates\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i + seq_length]\n",
    "        y = target[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    model.train()\n",
    "    average_losses_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        average_losses_per_epoch.append(avg_loss)\n",
    "        print(f'Epoch: {epoch+1}, Average Training Loss: {avg_loss:.6f}')\n",
    "    return model\n",
    "\n",
    "input_size = X_train_tanh.shape[1]\n",
    "lstm_hidden_size = 64\n",
    "num_layers = 1\n",
    "fc_hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "sequence_length = 24\n",
    "momentum = 0.9\n",
    "\n",
    "all_predictions_actuals = pd.DataFrame()\n",
    "monthly_metrics = []\n",
    "\n",
    "# Recalibration loop for each month from January to May 2024\n",
    "for month in range(1, 6):  \n",
    "    # Define the training and test periods\n",
    "    train_end_date = pd.to_datetime(f'2024-{month:02d}-01') - pd.DateOffset(days=1)\n",
    "    test_start_date = pd.to_datetime(f'2024-{month:02d}-01')\n",
    "    test_end_date = pd.to_datetime(f'2024-{month:02d}-01') + pd.DateOffset(months=1) - pd.DateOffset(hours=1)\n",
    "\n",
    "    train_start_date = train_end_date - pd.DateOffset(years=1) + pd.DateOffset(days=1)\n",
    "    \n",
    "    X_train_window = X_train[train_start_date:train_end_date]\n",
    "    y_train_window = y_train[train_start_date:train_end_date]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler_tanh.fit(X_train_window[non_dummy_columns])\n",
    "    scaler_target_tanh.fit(y_train_window.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_tanh = scaler_tanh.transform(X_train_window[non_dummy_columns])\n",
    "    y_train_tanh = scaler_target_tanh.transform(y_train_window.values.reshape(-1, 1))\n",
    "\n",
    "    X_train_tanh = pd.DataFrame(X_train_tanh, columns=non_dummy_columns, index=X_train_window.index)\n",
    "    X_train_tanh = pd.concat([X_train_tanh, X_train_window[dummy_columns]], axis=1)\n",
    "    y_train_tanh = pd.Series(y_train_tanh.flatten(), index=y_train_window.index)\n",
    "\n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_tanh.values, y_train_tanh.values, seq_length=sequence_length)\n",
    "    X_train_seq = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "    y_train_seq = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Reinitialize the model and optimizer for each month's training\n",
    "    model = LSTMModel(input_size=input_size, lstm_hidden_size=lstm_hidden_size, num_layers=num_layers, fc_hidden_size=fc_hidden_size)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "    # Prepare the test data\n",
    "    X_test_window = X_test[test_start_date:test_end_date]\n",
    "    y_test_window = y_test[test_start_date:test_end_date]\n",
    "\n",
    "    X_test_tanh = scaler_tanh.transform(X_test_window[non_dummy_columns])\n",
    "    y_test_tanh = scaler_target_tanh.transform(y_test_window.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_tanh = pd.DataFrame(X_test_tanh, columns=non_dummy_columns, index=X_test_window.index)\n",
    "    X_test_tanh = pd.concat([X_test_tanh, X_test_window[dummy_columns]], axis=1)\n",
    "    y_test_tanh = pd.Series(y_test_tanh.flatten(), index=y_test_window.index)\n",
    "\n",
    "    # Test the model\n",
    "    normalized_predictions, normalized_actuals, dates = test_model(trained_model, X_test_tanh, y_test_tanh, sequence_length, scaler_target_tanh)\n",
    "\n",
    "    # Rescale predictions and actuals\n",
    "    predictions_rescaled = rescale_values(scaler_target_tanh, np.array(normalized_predictions).reshape(-1, 1))\n",
    "    actuals_rescaled = rescale_values(scaler_target_tanh, np.array(normalized_actuals).reshape(-1, 1))\n",
    "\n",
    "    # Create DataFrames for predictions and actuals with datetime index\n",
    "    predictions_df = pd.DataFrame(predictions_rescaled, index=dates, columns=[\"Prediction\"])\n",
    "    actuals_df = pd.DataFrame(actuals_rescaled, index=dates, columns=[\"Actual\"])\n",
    "\n",
    "    # Combine predictions and actuals into a single DataFrame\n",
    "    predictions_actuals = pd.concat([predictions_df, actuals_df], axis=1)\n",
    "\n",
    "    # Append the current month's predictions and actuals to the combined DataFrame\n",
    "    all_predictions_actuals = pd.concat([all_predictions_actuals, predictions_actuals])\n",
    "\n",
    "    # Calculate and print performance metrics\n",
    "    mse, rmse, mae, mape = calculate_metrics(predictions_rescaled, actuals_rescaled)\n",
    "    smape = calculate_smape(predictions_rescaled, actuals_rescaled)\n",
    "    monthly_metrics.append((mse, rmse, mae, mape, smape))\n",
    "    print(f'{test_start_date.strftime(\"%B %Y\")} - Test Period: {test_start_date.date()} to {test_end_date.date()}')\n",
    "    print(f'MSE: {mse:.6f}')\n",
    "    print(f'RMSE: {rmse:.6f}')\n",
    "    print(f'MAE: {mae:.6f}')\n",
    "    print(f'MAPE: {mape:.6f}%')\n",
    "    print(f'SMAPE: {smape:.6f}%')\n",
    "\n",
    "for i, (mse, rmse, mae, mape, smape) in enumerate(monthly_metrics, 1):\n",
    "    print(f'Month {i}: MSE={mse:.6f}, RMSE={rmse:.6f}, MAE={mae:.6f}, MAPE={mape:.6f}%, SMAPE={smape:.6f}%')\n",
    "\n",
    "all_predictions_actuals.to_csv('all_predictions_actuals.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM predictions to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excel_file_path = '/Users/evenbakke/Documents/Master Thesis/Predictions all models /LSTM predictions.xlsx'\n",
    "\n",
    "all_predictions_actuals.to_excel(excel_file_path, index=True)\n",
    "\n",
    "print(f'DataFrame successfully saved to {excel_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
