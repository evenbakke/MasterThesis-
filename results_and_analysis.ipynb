{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/evenbakke/Documents/Master Thesis/Predictions all models /All predictions .xlsx')\n",
    "df.set_index(\"DateTime\", inplace=True)\n",
    "df.sort_index(ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error metrics for all models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def calculate_smape(predictions, actuals, epsilon=1e-10):\n",
    "    return 100 * np.mean(2 * np.abs(predictions - actuals) / (np.abs(actuals) + np.abs(predictions) + epsilon))\n",
    "\n",
    "def calculate_metrics(df, model_columns):\n",
    "    metrics = []\n",
    "    overall_metrics = {}\n",
    "\n",
    "    # Ensure the DataFrame is sorted by DateTime index\n",
    "    df = df.sort_index()\n",
    "\n",
    "    grouped = df.groupby([df.index.year, df.index.month])\n",
    "\n",
    "    # Initialize dictionaries to accumulate errors for overall metrics\n",
    "    overall_errors = {model: {'squared_errors': [], 'absolute_errors': [], 'smape_values': []} for model in model_columns}\n",
    "\n",
    "    for (year, month), group in grouped:\n",
    "        month_metrics = {'Year': year, 'Month': month}\n",
    "        actuals = group['Actual'].values\n",
    "\n",
    "        for model in model_columns:\n",
    "            predictions = group[model].values\n",
    "            rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "            mae = mean_absolute_error(actuals, predictions)\n",
    "            smape = calculate_smape(predictions, actuals)\n",
    "\n",
    "            month_metrics[f'{model} RMSE'] = rmse\n",
    "            month_metrics[f'{model} MAE'] = mae\n",
    "            month_metrics[f'{model} sMAPE'] = smape\n",
    "\n",
    "            # Accumulate errors for overall metrics\n",
    "            overall_errors[model]['squared_errors'].extend((actuals - predictions) ** 2)\n",
    "            overall_errors[model]['absolute_errors'].extend(np.abs(actuals - predictions))\n",
    "            overall_errors[model]['smape_values'].extend(2 * np.abs(actuals - predictions) / (np.abs(actuals) + np.abs(predictions) + 1e-10))\n",
    "\n",
    "        metrics.append(month_metrics)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    for model in model_columns:\n",
    "        overall_rmse = np.sqrt(np.mean(overall_errors[model]['squared_errors']))\n",
    "        overall_mae = np.mean(overall_errors[model]['absolute_errors'])\n",
    "        overall_smape = 100 * np.mean(overall_errors[model]['smape_values'])\n",
    "\n",
    "        overall_metrics[f'{model} RMSE'] = overall_rmse\n",
    "        overall_metrics[f'{model} MAE'] = overall_mae\n",
    "        overall_metrics[f'{model} sMAPE'] = overall_smape\n",
    "\n",
    "    return pd.DataFrame(metrics), overall_metrics\n",
    "\n",
    "model_columns = ['MLP Prediction', 'LSTM Prediction', 'ARIMA Prediction', 'SARIMA Prediction','Daily Seasonal Naive', 'Weekly Seasonal Naive']\n",
    "monthly_metrics, overall_metrics = calculate_metrics(df, model_columns)\n",
    "\n",
    "\n",
    "print(\"Monthly Metrics:\")\n",
    "print(monthly_metrics)\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "for model, metrics in overall_metrics.items():\n",
    "    print(f'{model}: {metrics}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diebold-Mariano test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "models = ['MLP Prediction', 'LSTM Prediction', 'ARIMA Prediction', \n",
    "          'SARIMA Prediction', 'Daily Seasonal Naive', 'Weekly Seasonal Naive']\n",
    "\n",
    "model_display_names = ['MLP', 'LSTM', 'ARIMA', 'SARIMA', 'Daily Naive', 'Weekly Naive']\n",
    "\n",
    "# Calculate forecast errors for each model\n",
    "for model in models:\n",
    "    df[f'Error_{model}'] = df['Actual'] - df[model]\n",
    "\n",
    "# Calculate the squared errors (MSE for each time point)\n",
    "for model in models:\n",
    "    df[f'MSE_{model}'] = df[f'Error_{model}'] ** 2\n",
    "\n",
    "# Function to calculate the Diebold–Mariano test statistic for a two-sided test\n",
    "def diebold_mariano_test(loss_diff):\n",
    "    mean_d = np.mean(loss_diff)\n",
    "    var_d = np.var(loss_diff, ddof=1)\n",
    "    dm_stat = mean_d / np.sqrt(var_d / len(loss_diff))\n",
    "    p_value = 2 * norm.cdf(-abs(dm_stat))\n",
    "    return dm_stat, p_value\n",
    "\n",
    "# Create a list to store results\n",
    "dm_results = []\n",
    "\n",
    "# Generate all pairs of models\n",
    "model_pairs = list(combinations(models, 2))\n",
    "\n",
    "# Compute the loss differentials for each pair of models\n",
    "for model_a, model_b in model_pairs:\n",
    "    df[f'Loss_Diff_{model_a}_{model_b}'] = df[f'MSE_{model_a}'] - df[f'MSE_{model_b}']\n",
    "\n",
    "# Perform DM test for each pair of models and print results\n",
    "for model_a, model_b in model_pairs:\n",
    "    loss_diff = df[f'Loss_Diff_{model_a}_{model_b}'].values\n",
    "    dm_stat, p_value = diebold_mariano_test(loss_diff)\n",
    "    dm_results.append({'Model_A': model_a, 'Model_B': model_b, 'DM_Stat': dm_stat, 'P_Value': p_value})\n",
    "    \n",
    "    # Print the results of the DM test\n",
    "    print(f\"Comparing {model_a} and {model_b}:\")\n",
    "    print(f\"  Diebold–Mariano Test Statistic: {dm_stat}\")\n",
    "    print(f\"  p-value: {p_value}\")\n",
    "    if p_value < 0.05:\n",
    "        if dm_stat < 0:\n",
    "            print(f\"  The {model_a} model performs significantly better than the {model_b} model.\")\n",
    "        else:\n",
    "            print(f\"  The {model_b} model performs significantly better than the {model_a} model.\")\n",
    "    else:\n",
    "        print(\"  The difference in forecast accuracy between the two models is not statistically significant.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "dm_results_df = pd.DataFrame(dm_results)\n",
    "\n",
    "dm_pivot = dm_results_df.pivot(index='Model_A', columns='Model_B', values='P_Value')\n",
    "\n",
    "dm_matrix = pd.DataFrame(np.nan, index=models, columns=models)\n",
    "\n",
    "for i, row in dm_pivot.iterrows():\n",
    "    for col in dm_pivot.columns:\n",
    "        dm_matrix.loc[i, col] = dm_pivot.loc[i, col]\n",
    "\n",
    "for i in range(dm_matrix.shape[0]):\n",
    "    for j in range(i, dm_matrix.shape[1]):\n",
    "        dm_matrix.iloc[j, i] = dm_matrix.iloc[i, j]\n",
    "\n",
    "# Rename the index and columns for display\n",
    "dm_matrix.index = model_display_names\n",
    "dm_matrix.columns = model_display_names\n",
    "\n",
    "mask = np.triu(np.ones_like(dm_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(dm_matrix, mask=mask, annot=True, fmt=\".1e\", cmap=\"coolwarm\", linewidths=.5, vmin=0, vmax=0.10)\n",
    "plt.xticks(rotation=0)  \n",
    "plt.yticks(rotation=0)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df['Hour'] = df.index.hour\n",
    "\n",
    "hourly_rmse_mlp = df.groupby('Hour').apply(lambda x: np.sqrt(((x['Actual'] - x['MLP Prediction']) ** 2).mean()))\n",
    "hourly_rmse_lstm = df.groupby('Hour').apply(lambda x: np.sqrt(((x['Actual'] - x['LSTM Prediction']) ** 2).mean()))\n",
    "\n",
    "hourly_mae_mlp = df.groupby('Hour').apply(lambda x: np.mean(np.abs(x['Actual'] - x['MLP Prediction'])))\n",
    "hourly_mae_lstm = df.groupby('Hour').apply(lambda x: np.mean(np.abs(x['Actual'] - x['LSTM Prediction'])))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "\n",
    "ax1.plot(hourly_rmse_mlp.index, hourly_rmse_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax1.plot(hourly_rmse_lstm.index, hourly_rmse_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax1.set_xlabel('Hour', fontsize=14)\n",
    "ax1.set_ylabel('RMSE error (EUR/MWh)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "ax2.plot(hourly_mae_mlp.index, hourly_mae_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax2.plot(hourly_mae_lstm.index, hourly_mae_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax2.set_xlabel('Hour', fontsize=14)\n",
    "ax2.set_ylabel('MAE error (EUR/MWh)', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Extract weekday (0=Monday, 6=Sunday)\n",
    "df['Weekday'] = df.index.weekday\n",
    "\n",
    "# Calculate RMSE for each weekday for MLP and LSTM\n",
    "weekday_rmse_mlp = df.groupby('Weekday').apply(lambda x: np.sqrt(((x['Actual'] - x['MLP Prediction']) ** 2).mean()))\n",
    "weekday_rmse_lstm = df.groupby('Weekday').apply(lambda x: np.sqrt(((x['Actual'] - x['LSTM Prediction']) ** 2).mean()))\n",
    "\n",
    "# Calculate MAE for each weekday for MLP and LSTM\n",
    "weekday_mae_mlp = df.groupby('Weekday').apply(lambda x: np.mean(np.abs(x['Actual'] - x['MLP Prediction'])))\n",
    "weekday_mae_lstm = df.groupby('Weekday').apply(lambda x: np.mean(np.abs(x['Actual'] - x['LSTM Prediction'])))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(weekday_rmse_mlp.index, weekday_rmse_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax1.plot(weekday_rmse_lstm.index, weekday_rmse_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax1.set_xlabel('Weekday', fontsize=14)\n",
    "ax1.set_ylabel('RMSE error (EUR/MWh)', fontsize=14)\n",
    "ax1.set_xticks(range(7))\n",
    "ax1.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(weekday_mae_mlp.index, weekday_mae_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax2.plot(weekday_mae_lstm.index, weekday_mae_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax2.set_xlabel('Weekday', fontsize=14)\n",
    "ax2.set_ylabel('MAE error (EUR/MWh)', fontsize=14)\n",
    "ax2.set_xticks(range(7))\n",
    "ax2.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Extract month (1=January, 12=December)\n",
    "df['Month'] = df.index.month\n",
    "\n",
    "# Calculate RMSE for each month for MLP and LSTM\n",
    "monthly_rmse_mlp = df.groupby('Month').apply(lambda x: np.sqrt(((x['Actual'] - x['MLP Prediction']) ** 2).mean()))\n",
    "monthly_rmse_lstm = df.groupby('Month').apply(lambda x: np.sqrt(((x['Actual'] - x['LSTM Prediction']) ** 2).mean()))\n",
    "\n",
    "# Calculate MAE for each month for MLP and LSTM\n",
    "monthly_mae_mlp = df.groupby('Month').apply(lambda x: np.mean(np.abs(x['Actual'] - x['MLP Prediction'])))\n",
    "monthly_mae_lstm = df.groupby('Month').apply(lambda x: np.mean(np.abs(x['Actual'] - x['LSTM Prediction'])))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(monthly_rmse_mlp.index, monthly_rmse_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax1.plot(monthly_rmse_lstm.index, monthly_rmse_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax1.set_xlabel('Month', fontsize=14)\n",
    "ax1.set_ylabel('RMSE error (EUR/MWh)', fontsize=14)\n",
    "ax1.set_xticks(range(1, 6))\n",
    "ax1.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(monthly_mae_mlp.index, monthly_mae_mlp, linestyle='-', label='MLP', color='blue')\n",
    "ax2.plot(monthly_mae_lstm.index, monthly_mae_lstm, linestyle='-', label='LSTM', color='orange')\n",
    "ax2.set_xlabel('Month', fontsize=14)\n",
    "ax2.set_ylabel('MAE error (EUR/MWh)', fontsize=14)\n",
    "ax2.set_xticks(range(1, 6))\n",
    "ax2.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting predictions from MLP and LSTM vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loop through the first five months\n",
    "for month in [1, 2, 3, 4, 5]:\n",
    "    # Filter data for the specific month\n",
    "    df_month = df[df.index.month == month]\n",
    "    \n",
    "    # Create a plot for the current month\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.plot(df_month.index, df_month['Actual'], label='Actual', color='blue', linewidth=1.5)\n",
    "    \n",
    "    plt.plot(df_month.index, df_month['MLP Prediction'], label='MLP Prediction', color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    plt.plot(df_month.index, df_month['LSTM Prediction'], label='LSTM Prediction', color='green', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    plt.title(f'Actual vs MLP and LSTM Predictions - Month {month}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_january = df[df.index.month == 1]\n",
    "\n",
    "for week in df_january.index.isocalendar().week.unique():\n",
    "\n",
    "    df_week = df_january[df_january.index.isocalendar().week == week]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.plot(df_week.index, df_week['Actual'], label='Actual', color='blue', linewidth=1.5)\n",
    "    \n",
    "    plt.plot(df_week.index, df_week['MLP Prediction'], label='MLP Prediction', color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    plt.plot(df_week.index, df_week['LSTM Prediction'], label='LSTM Prediction', color='green', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    plt.title(f'Actual vs MLP and LSTM Predictions - Week {week} of January')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions overestimation/underestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "over_under_results = {\n",
    "    'Model': [],\n",
    "    'Overestimate (%)': [],\n",
    "    'Underestimate (%)': []\n",
    "}\n",
    "\n",
    "models_to_investigate = ['MLP Prediction', 'LSTM Prediction']\n",
    "\n",
    "for model in models_to_investigate:\n",
    "    overestimates = (df[model] > df['Actual']).sum()\n",
    "    underestimates = (df[model] < df['Actual']).sum()\n",
    "    total_predictions = len(df)\n",
    "\n",
    "    overestimate_percentage = (overestimates / total_predictions) * 100\n",
    "    underestimate_percentage = (underestimates / total_predictions) * 100\n",
    "\n",
    "    over_under_results['Model'].append(model)\n",
    "    over_under_results['Overestimate (%)'].append(overestimate_percentage)\n",
    "    over_under_results['Underestimate (%)'].append(underestimate_percentage)\n",
    "\n",
    "over_under_df = pd.DataFrame(over_under_results)\n",
    "print(over_under_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagging positive and negative price spikes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Calculate the rolling mean and standard deviation with a window of 30 days\n",
    "rolling_mean = df['Actual'].rolling(window=30, min_periods=1).mean()\n",
    "rolling_std = df['Actual'].rolling(window=30, min_periods=1).std()\n",
    "\n",
    "# Define dynamic thresholds for positive and negative price spikes\n",
    "positive_spike_threshold = rolling_mean + 2 * rolling_std\n",
    "negative_spike_threshold = rolling_mean - 2 * rolling_std\n",
    "\n",
    "# Identify positive and negative price spikes\n",
    "positive_price_spikes = df[df['Actual'] > positive_spike_threshold]\n",
    "negative_price_spikes = df[df['Actual'] < negative_spike_threshold]\n",
    "\n",
    "# Loop through each month and generate a plot\n",
    "months = df.index.to_period('M').unique()\n",
    "\n",
    "for month in months:\n",
    "    monthly_data = df[df.index.to_period('M') == month]\n",
    "    monthly_positive_spikes = positive_price_spikes[positive_price_spikes.index.to_period('M') == month]\n",
    "    monthly_negative_spikes = negative_price_spikes[negative_price_spikes.index.to_period('M') == month]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(monthly_data.index, monthly_data['Actual'], label='Actual Prices', color='blue')\n",
    "    plt.plot(monthly_data.index, positive_spike_threshold[monthly_data.index], label='Positive Spike Threshold', color='orange', linestyle='--')\n",
    "    plt.plot(monthly_data.index, negative_spike_threshold[monthly_data.index], label='Negative Spike Threshold', color='green', linestyle='--')\n",
    "    plt.scatter(monthly_positive_spikes.index, monthly_positive_spikes['Actual'], color='red', label='Positive Spikes', marker='o')\n",
    "    plt.scatter(monthly_negative_spikes.index, monthly_negative_spikes['Actual'], color='purple', label='Negative Spikes', marker='x')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Electricity Prices with Price Spikes Highlighted - {month}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify positive and negative price spikes\n",
    "positive_price_spikes = df[df['Actual'] > positive_spike_threshold]\n",
    "negative_price_spikes = df[df['Actual'] < negative_spike_threshold]\n",
    "\n",
    "# Initialize a dictionary to store the results for MLP and LSTM\n",
    "spike_results = {\n",
    "    'Model': [],\n",
    "    'Overestimate Positive Spikes (%)': [],\n",
    "    'Underestimate Positive Spikes (%)': [],\n",
    "    'Overestimate Negative Spikes (%)': [],\n",
    "    'Underestimate Negative Spikes (%)': []\n",
    "}\n",
    "\n",
    "# List of models to investigate\n",
    "models_to_investigate = ['MLP Prediction', 'LSTM Prediction']\n",
    "\n",
    "# Calculate overestimation and underestimation percentages for each model for positive spikes\n",
    "for model in models_to_investigate:\n",
    "    overestimates_positive = (positive_price_spikes[model] > positive_price_spikes['Actual']).sum()\n",
    "    underestimates_positive = (positive_price_spikes[model] < positive_price_spikes['Actual']).sum()\n",
    "    total_positive_spikes = len(positive_price_spikes)\n",
    "    \n",
    "    overestimate_positive_percentage = (overestimates_positive / total_positive_spikes) * 100\n",
    "    underestimate_positive_percentage = (underestimates_positive / total_positive_spikes) * 100\n",
    "\n",
    "    overestimates_negative = (negative_price_spikes[model] > negative_price_spikes['Actual']).sum()\n",
    "    underestimates_negative = (negative_price_spikes[model] < negative_price_spikes['Actual']).sum()\n",
    "    total_negative_spikes = len(negative_price_spikes)\n",
    "    \n",
    "    overestimate_negative_percentage = (overestimates_negative / total_negative_spikes) * 100\n",
    "    underestimate_negative_percentage = (underestimates_negative / total_negative_spikes) * 100\n",
    "\n",
    "    spike_results['Model'].append(model)\n",
    "    spike_results['Overestimate Positive Spikes (%)'].append(overestimate_positive_percentage)\n",
    "    spike_results['Underestimate Positive Spikes (%)'].append(underestimate_positive_percentage)\n",
    "    spike_results['Overestimate Negative Spikes (%)'].append(overestimate_negative_percentage)\n",
    "    spike_results['Underestimate Negative Spikes (%)'].append(underestimate_negative_percentage)\n",
    "\n",
    "spike_results_df = pd.DataFrame(spike_results)\n",
    "print(spike_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Function to calculate RMSE and MAE\n",
    "def calculate_error_metrics(actual, predicted):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    return rmse, mae\n",
    "\n",
    "# Initialize a dictionary to store the results for MLP and LSTM\n",
    "spike_error_metrics = {\n",
    "    'Model': [],\n",
    "    'RMSE Positive Spikes': [],\n",
    "    'MAE Positive Spikes': [],\n",
    "    'RMSE Negative Spikes': [],\n",
    "    'MAE Negative Spikes': []\n",
    "}\n",
    "\n",
    "# Calculate RMSE and MAE for each model for positive and negative spikes\n",
    "for model in models_to_investigate:\n",
    "    # Positive Spikes\n",
    "    rmse_positive, mae_positive = calculate_error_metrics(positive_price_spikes['Actual'], positive_price_spikes[model])\n",
    "    \n",
    "    # Negative Spikes\n",
    "    rmse_negative, mae_negative = calculate_error_metrics(negative_price_spikes['Actual'], negative_price_spikes[model])\n",
    "    \n",
    "    spike_error_metrics['Model'].append(model)\n",
    "    spike_error_metrics['RMSE Positive Spikes'].append(rmse_positive)\n",
    "    spike_error_metrics['MAE Positive Spikes'].append(mae_positive)\n",
    "    spike_error_metrics['RMSE Negative Spikes'].append(rmse_negative)\n",
    "    spike_error_metrics['MAE Negative Spikes'].append(mae_negative)\n",
    "\n",
    "spike_error_metrics_df = pd.DataFrame(spike_error_metrics)\n",
    "print(spike_error_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction of price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "direction_accuracy = {\n",
    "    'Model': [],\n",
    "    'Correct Direction (%)': []\n",
    "}\n",
    "\n",
    "for model in models_to_investigate:\n",
    "    correct_direction = ((df['Actual'].diff() * df[model].diff()) > 0).sum()\n",
    "    total_direction_changes = len(df) - 1  \n",
    "    accuracy_percentage = (correct_direction / total_direction_changes) * 100\n",
    "\n",
    "    direction_accuracy['Model'].append(model)\n",
    "    direction_accuracy['Correct Direction (%)'].append(accuracy_percentage)\n",
    "\n",
    "direction_accuracy_df = pd.DataFrame(direction_accuracy)\n",
    "print(direction_accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "direction_accuracy = {\n",
    "    'Model': [],\n",
    "    'Correct Upward Change (%)': [],\n",
    "    'Correct Downward Change (%)': [],\n",
    "    'Overall Correct Direction (%)': []\n",
    "}\n",
    "\n",
    "models_to_investigate = ['MLP Prediction', 'LSTM Prediction']\n",
    "\n",
    "for model in models_to_investigate:\n",
    "    actual_diff = df['Actual'].diff()\n",
    "    model_diff = df[model].diff()\n",
    "    \n",
    "    # Upward changes\n",
    "    correct_upward_changes = ((actual_diff > 0) & (model_diff > 0)).sum()\n",
    "    total_upward_changes = (actual_diff > 0).sum()\n",
    "    upward_accuracy_percentage = (correct_upward_changes / total_upward_changes) * 100 if total_upward_changes > 0 else 0\n",
    "    \n",
    "    # Downward changes\n",
    "    correct_downward_changes = ((actual_diff < 0) & (model_diff < 0)).sum()\n",
    "    total_downward_changes = (actual_diff < 0).sum()\n",
    "    downward_accuracy_percentage = (correct_downward_changes / total_downward_changes) * 100 if total_downward_changes > 0 else 0\n",
    "    \n",
    "    # Overall direction accuracy\n",
    "    correct_direction = ((actual_diff * model_diff) > 0).sum()\n",
    "    total_direction_changes = len(df) - 1  \n",
    "    overall_accuracy_percentage = (correct_direction / total_direction_changes) * 100\n",
    "\n",
    "    direction_accuracy['Model'].append(model)\n",
    "    direction_accuracy['Correct Upward Change (%)'].append(upward_accuracy_percentage)\n",
    "    direction_accuracy['Correct Downward Change (%)'].append(downward_accuracy_percentage)\n",
    "    direction_accuracy['Overall Correct Direction (%)'].append(overall_accuracy_percentage)\n",
    "\n",
    "direction_accuracy_df = pd.DataFrame(direction_accuracy)\n",
    "print(direction_accuracy_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
